import pdfplumber
import re
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import numpy as np
import os
import csv
import faiss
import torch


def extract_text_from_pdf(pdf_path: str) -> str:

    full_text = ""

    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    full_text += text + "\n"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
        return ""

    return full_text


def clear_text(text: str) -> str:

    # --- 1. –ü–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ 'keyword(s)' / 'key word(s)'
    pattern_keywords = re.compile(r".*?key\s?word[s]?.*?\r?\n", re.IGNORECASE | re.DOTALL)
    text_after_keywords = pattern_keywords.sub("", text, count=1)
    if text_after_keywords != text:
        pattern_until_cyrillic = re.compile(r"^[^–∞-—è–ê-–Ø]+", re.DOTALL)
        text = pattern_until_cyrillic.sub("", text_after_keywords)
    else:
        # --- 2. –ü–æ–∏—Å–∫ '–≤–≤–µ–¥–µ–Ω–∏–µ' –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ –Ω–µ–≥–æ (–æ—Å—Ç–∞–≤–ª—è—è —Å—Ç—Ä–æ–∫—É)
        match_intro = re.search(r"–≤–≤–µ–¥–µ–Ω–∏–µ", text, re.IGNORECASE)
        if match_intro:
            text = text[match_intro.start():]
        else:
            # --- 3. –ü–æ–∏—Å–∫ '–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞' –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ –∏ –≤–∫–ª—é—á–∞—è —Å—Ç—Ä–æ–∫—É
            pattern_keywords_ru = re.compile(r".*?–∫–ª—é—á–µ–≤—ã–µ\s+—Å–ª–æ–≤–∞.*?\r?\n", re.IGNORECASE | re.DOTALL)
            text_after_kw_ru = pattern_keywords_ru.sub("", text, count=1)
            if text_after_kw_ru != text:
                text = text_after_kw_ru
            # –∏–Ω–∞—á–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Ç–µ–∫—Å—Ç –æ—Å—Ç–∞—ë—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å

    # --- 4. –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –≤—Ö–æ–∂–¥–µ–Ω–∏—é —Å–ø–∏—Å–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
    end_patterns = [
        r"—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã",
        r"–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π\s+—Å–ø–∏—Å–æ–∫",
        r"—Å–ø–∏—Å–æ–∫\s+–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
    ]
    combined_pattern = re.compile("|".join(end_patterns), re.IGNORECASE)

    matches = list(combined_pattern.finditer(text))
    if matches:
        last_match = matches[-1]
        start_of_line = text.rfind('\n', 0, last_match.start())
        if start_of_line == -1:
            # –µ—Å–ª–∏ –Ω–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å–∞ ‚Äî –æ—Ç –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–∞
            start_of_line = 0
        text = text[:start_of_line].rstrip()

    return text


def text_to_paragraphs(text: str):
    text = text.replace('ÔÄ≠', '')
    text = re.sub(r'-\s*\n\s*', '', text)
    text = re.sub(r'\r\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)

    parts = re.split(r'(?<=\.)\s*\n+', text)
    cleaned_paragraphs = []

    for p in parts:
        p = re.sub(r'(?m)^\s*.{1,4}\s*$\n?', '', p)
        p = re.sub(r'(?m)^\s*(–†–∏—Å\.|–ò—Å—Ç–æ—á–Ω–∏–∫:).*\n?', '', p)
        p = re.sub(r'\s*\n\s*', ' ', p).strip()
        if p:
            cleaned_paragraphs.append(p)

    merged = []
    for para in cleaned_paragraphs:
        if merged and para and para[0].islower():
            merged[-1] += ' ' + para
        else:
            merged.append(para)

    return merged


def split_sentences(paragraphs):
    sentences = []

    # –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è –∫–æ–Ω—Ü–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    abbreviations = [
        r'—Ç\.–¥\.', r'—Ç\.–ø\.', r'–∏\.–¥\.', r'–∏\.—Ç\.–¥\.', r'–∏\.—Ç\.–ø\.', r'–∏\.–¥—Ä\.',
        r'—Ç\.–µ\.', r'—Ç\.–Ω\.', r'–Ω–∞–ø—Ä\.', r'–≥\.', r'—É–ª\.', r'–¥\.',
        r'e\.g\.', r'i\.e\.', r'U\.S\.', r'Mr\.', r'Mrs\.', r'Dr\.'
    ]
    abbrev_pattern = '|'.join(abbreviations)

    for paragraph in paragraphs:
        # –ó–∞—â–∏—Ç–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
        safe_text = re.sub(f'({abbrev_pattern})', lambda m: m.group(1).replace('.', '¬ß'), paragraph)

        # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ .!? + –ø—Ä–æ–±–µ–ª + –∑–∞–≥–ª–∞–≤–Ω–∞—è –±—É–∫–≤–∞
        split_parts = re.split(r'(?<=[.!?])\s+(?=[–ê-–ØA-Z])', safe_text)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞—Ö –∏ —á–∏—Å—Ç–∏–º
        for part in split_parts:
            restored = part.replace('¬ß', '.').strip()
            if restored:
                sentences.append(restored)

    return sentences


def group_sentences_by_char_limit(sentences, limit=600):
    grouped = []
    current_group = []
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence)

        if current_length + sentence_length <= limit:
            current_group.append(sentence)
            current_length += sentence_length
        else:
            if current_group:
                grouped.append(' '.join(current_group))
            current_group = [sentence]
            current_length = sentence_length

    if current_group:
        grouped.append(' '.join(current_group))

    return grouped


def vectorize_paragraphs(paragraphs, model, tokenizer, max_tokens=512, batch_size=64):
    device = model.device
    filtered_paragraphs = []

    for i, paragraph in enumerate(paragraphs):
        tokens = tokenizer.encode(paragraph, add_special_tokens=True)
        token_len = len(tokens)

        if token_len <= max_tokens:
            filtered_paragraphs.append(paragraph)
        else:
            print(f"‚ö†Ô∏è –ê–±–∑–∞—Ü {i + 1} —Å–æ–¥–µ—Ä–∂–∏—Ç {token_len} —Ç–æ–∫–µ–Ω–æ–≤ –∏ –±—É–¥–µ—Ç —É—Å–µ—á—ë–Ω –¥–æ 512.")
            truncated = tokenizer.decode(tokens[:max_tokens], skip_special_tokens=True)
            filtered_paragraphs.append(truncated)

    if not filtered_paragraphs:
        return np.zeros(model.get_sentence_embedding_dimension())

    embeddings = model.encode(
        filtered_paragraphs,
        show_progress_bar=True,
        convert_to_numpy=True,
        device=device,
        batch_size=batch_size
    )

    doc_embedding = np.mean(embeddings, axis=0)
    return doc_embedding



def find_most_similar(embeddings_list, filenames, top_k=5, max_outputs=5):

    embeddings_np = np.stack(embeddings_list).astype("float32")
    index = faiss.IndexFlatL2(embeddings_np.shape[1])
    index.add(embeddings_np)

    def print_results(base_idx, distances, indices):
        print(f"\nüîç –ü–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ñ–∞–π–ª: {filenames[base_idx]}")
        printed = 0
        for i, dist in zip(indices, distances):
            if i == base_idx:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º —Å–µ–±—è
            printed += 1
            print(f"{printed}. {filenames[i]} ‚Äî —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {dist:.4f}")
            if printed >= top_k:
                break

    for i in range(min(len(embeddings_list), max_outputs)):
        distances, indices = index.search(embeddings_np[i:i+1], top_k + 1)
        print_results(i, distances[0], indices[0])



def export_articles_to_txt(text, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        for paragraph in text:
            f.write(paragraph.strip() + '\n\n')


def export_embeddings_to_csv(embeddings_list, output_path):
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['embedding'])
        for emb in embeddings_list:
            writer.writerow([', '.join(map(str, emb))])


def main(max_files=None):
    pdf_folder = "downloads/pdfs"
    output_csv = "all_embeddings.csv"

    embeddings_list = []
    filenames = []

    model_name = 'sentence-transformers/all-MiniLM-L6-v2'
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"\nüß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {device.upper()}...")
    model = SentenceTransformer(model_name)
    model.to(torch.device(device))
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    pdf_files = [
        f for f in os.listdir(pdf_folder)
        if f.lower().endswith('.pdf') and os.path.splitext(f)[0].isdigit()
    ]
    pdf_files.sort(key=lambda x: int(os.path.splitext(x)[0]))

    if max_files is not None:
        pdf_files = pdf_files[:max_files]

    for filename in pdf_files:
        file_path = os.path.join(pdf_folder, filename)
        print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")

        text = extract_text_from_pdf(file_path)
        if not text.strip():
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª (–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞): {filename}")
            continue

        cleared_text = clear_text(text)
        paragraphs = text_to_paragraphs(cleared_text)
        sentences = split_sentences(paragraphs)
        grouped = group_sentences_by_char_limit(sentences)
        doc_embedding = vectorize_paragraphs(grouped, model, tokenizer)

        embeddings_list.append(doc_embedding)
        filenames.append(filename)

    if embeddings_list:
        export_embeddings_to_csv(embeddings_list, output_csv)
        print(f"\n‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_csv}")

        # –í—ã–≤–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        find_most_similar(embeddings_list, filenames, top_k=5)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.")



if __name__ == "__main__":
    main(max_files = 1000)
